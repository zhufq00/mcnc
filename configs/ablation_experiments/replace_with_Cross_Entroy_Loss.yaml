encode_max_length: 100
eval_decode_max_length: 30
truncation: True
model_type: 'bart_mask_random'
pretrained_model_path: 'bart-base'
data_dir: './data/negg_data'
annotation: 'replace_with_Cross_Entroy_Loss'
checkpoint : 'TODO' # TODO
resume: True
use_gpu: True
multi_gpu: False
debug: False
pro_type : 'sqrt'
gpuid: '0'
gpu_num: 1
eval: False
test: True
filemode: 'w'
patience : 5
num_train_epochs: 100
max_train_steps: 0
per_gpu_train_batch_size: 64
gradient_accumulation_steps: 1
eval_batch_size: 64
log_step: 10
margin: 0.5
noise_lambda: 0
denominator_correction_factor : 0
pretrain: False
random_span: False
softmax: True
lr_scheduler_type: 'constant' # constant cosine
loss_fct: 'CrossEntropyLoss' # CrossEntropyLoss MarginRankingLoss 
beta : 1
fp16: False
lr: 1.0e-5
weight_decay: 1.0e-6  # 1.0e-6 #1e-2
epsilon: 1.0e-8
vocab_size: 50265
seed: 2022